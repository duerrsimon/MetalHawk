{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3907e685",
   "metadata": {},
   "source": [
    "# Training the ANN models\n",
    "\n",
    "## Description\n",
    "\n",
    "This notebook contains the code to train and optimize the ANN models on the CSD (and PDB) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80c7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports.\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the source code directory in the search path.\n",
    "module_path = os.path.abspath(os.path.join(\"../src/\"))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "# _end_if_\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import h5py\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from metal_auxiliaries import METAL_TARGETS, MetalAtom\n",
    "from metal_pdb_data import MetalPdbData\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Categorical, Integer, Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccf752f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of class targets:\n",
    "# 1) the first value corresponds to the numerical target\n",
    "# 2) and the second to its number of coordinates\n",
    "CLASS_TARGETS = {\n",
    "    \"LIN\": (0, 2),\n",
    "    \"TRI\": (1, 3),\n",
    "    \"TET\": (2, 4),\n",
    "    \"SPL\": (3, 4),\n",
    "    \"SQP\": (4, 5),\n",
    "    \"TBP\": (5, 5),\n",
    "    \"OCT\": (6, 6),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c15b559",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data locations\n",
    "\n",
    "Here we set the directories where the data are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138baf63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CSD data: size=(109001, 37)\n",
    "csd_path = Path(\"../data/CSD/\")\n",
    "\n",
    "# PDB data: size=(2960, 37)\n",
    "pdb_path = Path(\"../data/PDB/\")\n",
    "\n",
    "# Output (temporary) directory.\n",
    "output_path = Path(\"../results/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13d858e-549e-4c74-808a-140f8736786e",
   "metadata": {},
   "source": [
    "## Load the data for ML algorithms\n",
    "\n",
    "Load the data and split them in train/test. The limit is usually set at 10% (for the test size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7256cc-883f-4ff2-aa8a-3eb461d9c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the trainning data type: {CSD or PDB}\n",
    "train_type = \"CSD\"\n",
    "\n",
    "# Complete structure.\n",
    "data_set = {\"Train\": None, \"Test\": None}\n",
    "\n",
    "# Test size ~ 10%.\n",
    "test_split = 0.10\n",
    "\n",
    "# Load the data files.\n",
    "if train_type == \"CSD\":\n",
    "\n",
    "    # Load CSD data.\n",
    "    with h5py.File(Path(\"../data/csd_metal_data.h5\"), mode=\"r\") as hdf_file:\n",
    "\n",
    "        # Make sure they are numpy arrays.\n",
    "        CSD_DATA = np.array(hdf_file[\"data\"], dtype=float)\n",
    "    # _end_with_\n",
    "\n",
    "    # Total number of test points.\n",
    "    test_size = int(len(CSD_DATA) * test_split)\n",
    "\n",
    "    # Split test set.\n",
    "    data_set[\"Test\"] = CSD_DATA[0:test_size, :]\n",
    "\n",
    "    # Split train set.\n",
    "    data_set[\"Train\"] = CSD_DATA[test_size:, :]\n",
    "\n",
    "else:\n",
    "\n",
    "    # Load PDB data.\n",
    "    with h5py.File(Path(\"../data/pdb_metal_data.h5\"), mode=\"r\") as hdf_file:\n",
    "\n",
    "        # Make sure they are numpy arrays.\n",
    "        PDB_DATA = np.array(hdf_file[\"data\"], dtype=float)\n",
    "    # _end_with_\n",
    "\n",
    "    # Total number of test points.\n",
    "    test_size = int(len(PDB_DATA) * test_split)\n",
    "\n",
    "    # Split test set.\n",
    "    data_set[\"Test\"] = PDB_DATA[0:test_size, :]\n",
    "\n",
    "    # Split train set.\n",
    "    data_set[\"Train\"] = PDB_DATA[test_size:, :]\n",
    "# _end_if_\n",
    "\n",
    "# Split test/train data.\n",
    "x_train = data_set[\"Train\"][:, 0:-1].copy()\n",
    "x_test = data_set[\"Test\"][:, 0:-1].copy()\n",
    "\n",
    "y_train = data_set[\"Train\"][:, -1].copy()\n",
    "y_test = data_set[\"Test\"][:, -1].copy()\n",
    "\n",
    "# Print the dimensions of the datasets.\n",
    "print(f\"{train_type} --> {train_type}\")\n",
    "print(f\"Dataset sizes: \")\n",
    "print(f\"X-train: {x_train.shape}, y-train: {y_train.shape}\")\n",
    "print(f\"X-test : {x_test.shape}, y-test: {y_test.shape}\")\n",
    "print(\"Done!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e10d00d-f0b1-4818-8185-79868f8701f2",
   "metadata": {},
   "source": [
    "## Bayesian Hyper-Parameter Optimization\n",
    "\n",
    "Setup a search grid and perform Bayesian HPO (using Gaussian Processes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb9e47-7220-490a-bc6e-64e200740300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the search space for the HPO.\n",
    "search_space = {\n",
    "    \"mlp__activation\": Categorical([\"relu\", \"tanh\"]),\n",
    "    \"mlp__hidden_layer_sizes\": Integer(10, 1024),\n",
    "    \"mlp__shuffle\": Categorical([True, False]),\n",
    "    \"mlp__alpha\": Real(1.0e-4, 1.0e-1, prior=\"log-uniform\"),\n",
    "    \"mlp__learning_rate_init\": Real(1.0e-3, 1.0e-1, prior=\"log-uniform\"),\n",
    "    \"mlp__batch_size\": Integer(16, 1024),\n",
    "}\n",
    "\n",
    "# Create the HPO pipeline.\n",
    "hpo_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"data_scaler\", RobustScaler(copy=True)),\n",
    "        (\"mlp\", MLPClassifier(solver=\"adam\", max_iter=200, early_stopping=False)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Search across different combinations using Bayesian optimization.\n",
    "hpo_search = BayesSearchCV(\n",
    "    estimator=hpo_pipeline,\n",
    "    search_spaces=search_space,\n",
    "    refit=True,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    verbose=5,\n",
    "    n_jobs=5,\n",
    "    return_train_score=True,\n",
    "    random_state=911,\n",
    "    error_score=np.nan,\n",
    ")\n",
    "\n",
    "# Initial time.\n",
    "t0 = perf_counter()\n",
    "\n",
    "# Display the model we are optimizing.\n",
    "print(f\"Optimizing ANN model ...\\n\")\n",
    "\n",
    "# Fit the HPO search model.\n",
    "hpo_search.fit(x_train, y_train)\n",
    "\n",
    "# Final time.\n",
    "tf = perf_counter()\n",
    "\n",
    "# Display information.\n",
    "print(f\"HPO finished in {tf-t0:.2f} seconds.\\n\")\n",
    "\n",
    "# Display best parameters.\n",
    "print(f\" Best score {hpo_search.best_score_}, with parameters are: \\n\")\n",
    "for b_param, b_value in hpo_search.best_params_.items():\n",
    "    print(f\" {b_param} --> {b_value}\")\n",
    "# _end_for_\n",
    "\n",
    "# Save the best model.\n",
    "joblib.dump(\n",
    "    hpo_search.best_estimator_,\n",
    "    Path(f\"../models/HPO_{train_type}_{train_type}_CV.model\"),\n",
    "    compress=\"zlib\",\n",
    ")\n",
    "\n",
    "# Display final message.\n",
    "print(\" Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e88c1-d8c6-4ab5-a911-4aaf09b79b0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75d9b0fb-049e-4e5f-8f17-d870a65aa6d3",
   "metadata": {},
   "source": [
    "## Cross-Validation Score\n",
    "\n",
    "This step is not necessary, but it can help to verify the results of the HPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51968039-ff68-404c-9a0f-b976d4b6fb41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load an HPO model.\n",
    "clf_model = joblib.load(Path(f\"../models/HPO_{train_type}_{train_type}_CV.model\"))\n",
    "\n",
    "# First time.\n",
    "t0 = perf_counter()\n",
    "\n",
    "# Run the K-Fold cross-validation.\n",
    "cv_scores = cross_val_score(\n",
    "    clf_model, x_train, y_train, scoring=\"balanced_accuracy\", n_jobs=5, verbose=0, cv=5\n",
    ")\n",
    "\n",
    "# Final time.\n",
    "tf = perf_counter()\n",
    "\n",
    "# Display information.\n",
    "print(f\"Finished K-Fold cross-validation in {tf-t0:.2f} seconds.\\n\")\n",
    "\n",
    "# Print the scores.\n",
    "print(\"Cross-validation Scores: \")\n",
    "for i, sc in enumerate(cv_scores, start=1):\n",
    "    print(f\"Fold: {i}, Score= {sc}\")\n",
    "# _end_for_\n",
    "\n",
    "print(f\"Mean={cv_scores.mean():.4f}, STD= {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e30f1-151b-4b73-84b7-de148e3e496a",
   "metadata": {},
   "source": [
    "## Validate the model on the test data\n",
    "\n",
    "Once a model has been optimized we can validate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9155d5-3257-42ac-b2c6-b9dbb39e8ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First time.\n",
    "t0 = perf_counter()\n",
    "\n",
    "# Get the predictions on the test set.\n",
    "y_pred = clf_model.predict(x_test)\n",
    "\n",
    "# Last time.\n",
    "t1 = perf_counter()\n",
    "\n",
    "# Display information.\n",
    "print(f\"Finished predicting test set: {x_test.shape}, in {t1-t0:.2f} seconds.\\n\")\n",
    "\n",
    "# Get the final report.\n",
    "clf_rep = classification_report(\n",
    "    y_test, y_pred, output_dict=True, target_names=CLASS_TARGETS.keys()\n",
    ")\n",
    "\n",
    "# Create a large figure.\n",
    "_, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# Create a Confusion matrix.\n",
    "matrix = ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    ax=ax,\n",
    "    cmap=plt.cm.GnBu,\n",
    "    xticks_rotation=45.0,\n",
    "    normalize=\"true\",\n",
    "    display_labels=CLASS_TARGETS.keys(),\n",
    "    values_format=\".2f\",\n",
    "    colorbar=False,\n",
    ")\n",
    "\n",
    "# Setup the title.\n",
    "ax.set_title(\n",
    "    \"Balanced Accuracy = {0:.2f}%, F1-weighted = {1:.2f}%\".format(\n",
    "        100.0 * balanced_accuracy_score(y_test, y_pred),\n",
    "        100.0 * clf_rep[\"weighted avg\"][\"f1-score\"],\n",
    "    ),\n",
    "    fontsize=18,\n",
    ")\n",
    "ax.set_xlabel(\"Predicted label\", fontsize=16)\n",
    "ax.set_ylabel(\"True label\", fontsize=16)\n",
    "\n",
    "# Set the custom colorbar.\n",
    "matrix.im_.autoscale()\n",
    "plt.colorbar(matrix.im_, fraction=0.046, pad=0.04)\n",
    "\n",
    "# Save the figure.\n",
    "plt.savefig(\n",
    "    Path(output_path / \"plots\" / f\"{train_type}_{train_type}_confusion_matrix.png\"),\n",
    "    bbox_inches=\"tight\",\n",
    "    dpi=300,\n",
    "    facecolor=\"w\",\n",
    "    edgecolor=\"w\",\n",
    "    orientation=\"landscape\",\n",
    ")\n",
    "\n",
    "# Save the classification to a json file.\n",
    "with open(\n",
    "    Path(\n",
    "        output_path\n",
    "        / \"reports\"\n",
    "        / f\"{train_type}_{train_type}_classification_report.json\"\n",
    "    ),\n",
    "    mode=\"w\",\n",
    ") as json_file:\n",
    "    json.dump(clf_rep, json_file, indent=4)\n",
    "# _end_with_\n",
    "\n",
    "# Get the loss-curve.\n",
    "loss_curve = clf_model[\"mlp\"].loss_curve_\n",
    "\n",
    "# Plot the figure.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(loss_curve)\n",
    "ax.set_title(f\"Multi Layer Perceptron Classifier\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the figure.\n",
    "plt.savefig(\n",
    "    Path(output_path / \"plots\" / f\"{train_type}_{train_type}_loss_curve.png\"),\n",
    "    bbox_inches=\"tight\",\n",
    "    facecolor=\"w\",\n",
    "    edgecolor=\"w\",\n",
    "    dpi=300,\n",
    "    orientation=\"landscape\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75b835",
   "metadata": {},
   "source": [
    "### End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e1398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_25",
   "language": "python",
   "name": "tensorflow_25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

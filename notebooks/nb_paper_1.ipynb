{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3907e685",
   "metadata": {},
   "source": [
    "# Paper no.1\n",
    "\n",
    "## Description\n",
    "\n",
    "This notebook contains material for the first paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e80c7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports.\n",
    "import os, sys\n",
    "\n",
    "# Add the source code directory in the search path.\n",
    "module_path = os.path.abspath(os.path.join('../src/'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "# _end_if_\n",
    "\n",
    "import h5py\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from time import perf_counter\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             confusion_matrix,\n",
    "                             RocCurveDisplay,\n",
    "                             classification_report,\n",
    "                             ConfusionMatrixDisplay,\n",
    "                             balanced_accuracy_score)\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import (Real,\n",
    "                         Integer,\n",
    "                         Categorical)\n",
    "\n",
    "# Custom code imports.\n",
    "from metal_pdb_data import MetalPdbData\n",
    "from metal_auxiliaries import MetalAtom, METAL_TARGETS\n",
    "from dual_output_classification import DualOutputClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31376cf-81a0-4005-b94f-2cd1ffa7430a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf752f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of class targets:\n",
    "# 1) the first value corresponds to the numerical target\n",
    "# 2) and the second to its number of coordinates\n",
    "CLASS_TARGETS = {\"LIN\": (0, 2),\n",
    "                 \"TRI\": (1, 3),\n",
    "                 \"TET\": (2, 4),\n",
    "                 \"SPL\": (3, 4),\n",
    "                 \"SQP\": (4, 5),\n",
    "                 \"TBP\": (5, 5),                 \n",
    "                 \"OCT\": (6, 6)}\n",
    "\n",
    "# List of classes (+ coordinates systems).\n",
    "LIST_OF_CLASS = [(\"lin\", \"2_coord\"),\n",
    "                 (\"tri\", \"3_coord\"),\n",
    "                 (\"tet\", \"4_coord\"),\n",
    "                 (\"spl\", \"4_coord\"),\n",
    "                 (\"sqp\", \"5_coord\"),\n",
    "                 (\"tbp\", \"5_coord\"),\n",
    "                 (\"oct\", \"6_coord\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c15b559",
   "metadata": {},
   "source": [
    "## Data locations:\n",
    "\n",
    "Sets the directories where the datafiles are stored. These are HDF files that include:\n",
    "\n",
    "1. the extracted angles\n",
    "2. the distance maps\n",
    "3. the file-ids\n",
    "4. the minimum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138baf63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CSD (training) data.\n",
    "# Size: (109001, 37)\n",
    "csd_path = Path(\"../data/CSD/\")\n",
    "\n",
    "# PDB (testing) data.\n",
    "# Size: (2960, 37)\n",
    "pdb_path = Path(\"../data/PDB/\")\n",
    "\n",
    "# Output (temporary) directory.\n",
    "output_path = Path(\"../results/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3149870-c475-4e22-a121-95b3f2823397",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract angles and contact maps and create the input data from the '.h5' files:\n",
    "\n",
    "Here we read all the '.h5' files and create the required data files, that will be\n",
    "used from the machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c63f820-7a72-48d2-8a1a-2898c13496ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a random number generator.\n",
    "rng = np.random.default_rng(20220427)\n",
    "\n",
    "# Set the directory you want to process:\n",
    "# 1) csd_path\n",
    "# 2) pdb_path\n",
    "parent_dir = csd_path\n",
    "\n",
    "# Localize the triu_indices method.\n",
    "np_triu_indices = np.triu_indices\n",
    "\n",
    "# Store the data for each class.\n",
    "data_vectors = []\n",
    "\n",
    "# Counter the instances of each class.\n",
    "class_counter = defaultdict(int)\n",
    "\n",
    "# Process all files in the path.\n",
    "for f_path in sorted([*parent_dir.glob(\"*.h5\")]):\n",
    "    \n",
    "    # Split the filename.\n",
    "    fname_split = f_path.stem.split(\"_\")\n",
    "    \n",
    "    # Class label.\n",
    "    class_label = str(fname_split[-1])\n",
    "    \n",
    "    # Load the data file.\n",
    "    with h5py.File(f_path, \"r\") as data_file:\n",
    "        \n",
    "        # Convert directly to numpy.\n",
    "        metal_dist_map = np.array(data_file[\"dist_map\"], dtype=float)\n",
    "        \n",
    "        # Convert directly to numpy.\n",
    "        metal_angles = np.array(data_file[\"angles\"], dtype=float)        \n",
    "    # _end_with_\n",
    "        \n",
    "    # Process all vectors for the class.\n",
    "    for arr_i, ang_i in tqdm(zip(metal_dist_map, metal_angles),\n",
    "                             f\" Processing data for -> Class = {class_label} \"): \n",
    "        \n",
    "        # Count the class.\n",
    "        class_counter[class_label] += 1\n",
    "        \n",
    "        # Total input vector.\n",
    "        xx_total = []\n",
    "        \n",
    "        # Size of matrix.\n",
    "        L = arr_i.shape[0]\n",
    "\n",
    "        # Get the upper triangular values.\n",
    "        xx_1 = arr_i[np_triu_indices(L, k=1)]\n",
    "\n",
    "        # Augment the upper triangular matrix.\n",
    "        xx_total.extend(xx_1)\n",
    "                    \n",
    "        # Augment the angles vector.\n",
    "        xx_total.extend(ang_i)\n",
    "        \n",
    "        # Add the target value.\n",
    "        xx_total.append(CLASS_TARGETS[class_label][0])\n",
    "        \n",
    "        # Sanity check.\n",
    "        if len(xx_total) == 0:\n",
    "            \n",
    "            # Something went wrong.\n",
    "            raise RuntimeError(\" Something went wrong. You need data ... \")\n",
    "        # _end_if_ \n",
    "        \n",
    "        # Append the complete vector.\n",
    "        data_vectors.append(xx_total)\n",
    "        \n",
    "    # _end_for_\n",
    "    \n",
    "# _end_for_\n",
    "\n",
    "# Convert the dataset to numpy array.\n",
    "data = np.array(data_vectors, dtype=float)\n",
    "\n",
    "# Shuffle the data.\n",
    "rng.shuffle(data)\n",
    "\n",
    "# Print the dimensions of the datasets.\n",
    "print(f\" Dataset size: {data.shape} \\n\")\n",
    "\n",
    "# Create an output path.\n",
    "path_out = Path(\"../data/csd_metal_data.h5\")\n",
    "\n",
    "# Create a HDF5 output file (write only).\n",
    "with h5py.File(path_out, \"w\") as f_out:\n",
    "    \n",
    "    # Save the data to disk.\n",
    "    f_out.create_dataset(\"data\", data=data, dtype=float)\n",
    "# _end_with_\n",
    "\n",
    "# Final confirmation.\n",
    "print(\" Done! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277a01bf-9710-4c79-b3cf-121241374c0e",
   "metadata": {},
   "source": [
    "## Make some useful plots from the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f132faff-b2ad-4b79-86d3-c2feccfa6109",
   "metadata": {},
   "source": [
    "### 1. Plot the _pie distribution_ of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee5759-ec6b-4ccc-a696-f01f8b7c89eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the type of data.\n",
    "head_class = \"CSD\" # PDB\n",
    "\n",
    "# Change the font family.\n",
    "plt.rcParams['font.family'] = 'monospace'\n",
    "\n",
    "# Number of coordinates.\n",
    "num_coord_sizes = {\"2-coord\": class_counter[\"LIN\"],\n",
    "                   \n",
    "                   \"3-coord\": class_counter[\"TRI\"],\n",
    "                   \n",
    "                   \"4-coord\": class_counter[\"SPL\"] + class_counter[\"TET\"],\n",
    "                   \n",
    "                   \"5-coord\": class_counter[\"TBP\"] + class_counter[\"SQP\"],\n",
    "                   \n",
    "                   \"6-coord\": class_counter[\"OCT\"]}\n",
    "\n",
    "# Create a new figure.\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# Select a Qualitative color mpa with enough colours.\n",
    "cmap = plt.get_cmap(\"tab20c\")\n",
    "\n",
    "# The outer-circle (num-coords).\n",
    "outer_colors = cmap([0, 4, 8, 12, 16])\n",
    "\n",
    "# The inner-circle (geom-classes).\n",
    "inner_colors = cmap([1,\n",
    "                     6,\n",
    "                     9, 11,\n",
    "                     13, 15, \n",
    "                     18])\n",
    "\n",
    "# Adjust the radius.\n",
    "size = 0.25\n",
    "\n",
    "# First plot the outer-circle.\n",
    "ax.pie(num_coord_sizes.values(), radius=1.0,\n",
    "       colors=outer_colors, wedgeprops=dict(width=size, edgecolor='w'),\n",
    "       labels=num_coord_sizes.keys(), labeldistance=0.8)\n",
    "\n",
    "# Then plot the inner-circle.\n",
    "ax.pie(class_counter.values(), radius=1.0-size,\n",
    "       colors=inner_colors, wedgeprops=dict(width=size, edgecolor='w'),\n",
    "       labels=class_counter.keys(), labeldistance=0.8)\n",
    "\n",
    "# Calculate the percentage of each class.\n",
    "tot = sum(class_counter.values())\n",
    "\n",
    "# Final string.\n",
    "str_ff = \"Percentages: \\n\"\n",
    "\n",
    "for x, y in class_counter.items():\n",
    "    \n",
    "    pct = np.round(y / tot*100.0, 2)\n",
    "    \n",
    "    # str_ff += \"{0:}:{1:>6}% \\n\".format(x, pct)\n",
    "    str_ff += \"{0:}: {1:5.2f}% \\n\".format(x, pct)\n",
    "# _end_for_\n",
    "\n",
    "# Add the legend.\n",
    "ax.text(1.1, -1.0, str_ff, size=20)\n",
    "\n",
    "# Title.\n",
    "ax.set_title(f\"{head_class} (metal) files\", fontsize=24)\n",
    "\n",
    "# Make the axis symmetric.\n",
    "ax.axis('equal')\n",
    "\n",
    "# Save the figure.\n",
    "plt.savefig(Path(output_path/\"plots\"/f\"{head_class}_Class_Pie.png\"), bbox_inches=\"tight\",\n",
    "            dpi=300, facecolor='w', edgecolor='w', orientation='landscape')\n",
    "\n",
    "# Change the font family.\n",
    "plt.rcParams['font.family'] = 'sans'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13d858e-549e-4c74-808a-140f8736786e",
   "metadata": {},
   "source": [
    "## Load the data for ML algorithms:\n",
    "\n",
    "TBD..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da7256cc-883f-4ff2-aa8a-3eb461d9c0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSD --> CSD\n",
      "Dataset sizes: \n",
      "X-train: (98101, 36), y-train: (98101,), Missing values: 0.014%\n",
      "X-test : (10900, 36), y-test: (10900,), Missing values: 0.015%\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the train/test type.\n",
    "train_type = \"CSD\" # {CSD or PDB}\n",
    "test_type = \"CSD\"  # {CSD or PDB}\n",
    "\n",
    "# Complete structure.\n",
    "data_set = {\"Train\": None, \"Test\": None}\n",
    "\n",
    "# Test size ~ 10%.\n",
    "test_split = 0.10\n",
    "\n",
    "# Load the TRAIN data files.\n",
    "if train_type == \"CSD\":\n",
    "    \n",
    "    # Load train datasets.\n",
    "    with h5py.File(Path(\"../data/csd_metal_data.h5\"),\n",
    "                   mode=\"r\") as hdf_file:\n",
    "        \n",
    "        # Load the data.\n",
    "        CSD_DATA = np.array(hdf_file[\"data\"], dtype=float)\n",
    "    # _end_with_\n",
    "    \n",
    "    # Check the test set type.\n",
    "    if test_type == \"CSD\":\n",
    "        \n",
    "        # Total number of test points.\n",
    "        test_size = int(len(CSD_DATA) * test_split)\n",
    "\n",
    "        # Split test set.\n",
    "        data_set[\"Test\"] = CSD_DATA[0:test_size, :]\n",
    "\n",
    "        # Split train set.\n",
    "        data_set[\"Train\"] = CSD_DATA[test_size:, :]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Load PDB data.\n",
    "        with h5py.File(Path(\"../data/pdb_metal_data.h5\"),\n",
    "                       mode=\"r\") as hdf_file:\n",
    "\n",
    "            # Load the data.\n",
    "            PDB_DATA = np.array(hdf_file[\"data\"], dtype=float)\n",
    "        # _end_with_\n",
    "        \n",
    "        # Put the data together.\n",
    "        data_set = {\"Train\": CSD_DATA, \"Test\": PDB_DATA}\n",
    "    # _end_if_\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # Load PDB data.\n",
    "    with h5py.File(Path(\"../data/pdb_metal_data.h5\"),\n",
    "                   mode=\"r\") as hdf_file:\n",
    "\n",
    "        # Load the data.\n",
    "        PDB_DATA = np.array(hdf_file[\"data\"], dtype=float)\n",
    "    # _end_with_\n",
    "    \n",
    "    # Total number of test points.\n",
    "    test_size = int(len(PDB_DATA) * test_split)\n",
    "\n",
    "    # Split test set.\n",
    "    data_set[\"Test\"] = PDB_DATA[0:test_size, :]\n",
    "\n",
    "    # Split train set.\n",
    "    data_set[\"Train\"] = PDB_DATA[test_size:, :]\n",
    "# _end_if_\n",
    "\n",
    "# Split test/train data.\n",
    "x_train = data_set[\"Train\"][:, 0:-1].copy()\n",
    "x_test = data_set[\"Test\"][:, 0:-1].copy()\n",
    "\n",
    "y_train = data_set[\"Train\"][:, -1].copy()\n",
    "y_test = data_set[\"Test\"][:, -1].copy()\n",
    "\n",
    "# Compute the missing values int the two sets.\n",
    "train_missing = np.round(100.0 * np.sum(x_train == -1.0)/x_train.size, 3)\n",
    "test_missing = np.round(100.0 * np.sum(x_test == -1.0)/x_test.size, 3)\n",
    "\n",
    "# Print the dimensions of the datasets.\n",
    "print(f\"{train_type} --> {test_type}\")\n",
    "print(f\"Dataset sizes: \")\n",
    "print(f\"X-train: {x_train.shape}, y-train: {y_train.shape}, Missing values: {train_missing}%\")\n",
    "print(f\"X-test : {x_test.shape}, y-test: {y_test.shape}, Missing values: {test_missing}%\")\n",
    "print(\"Done!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e10d00d-f0b1-4818-8185-79868f8701f2",
   "metadata": {},
   "source": [
    "## Bayesian HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb9e47-7220-490a-bc6e-64e200740300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the search space for the HPO.\n",
    "search_space = {\"mlp__activation\": Categorical(['relu', 'tanh']),\n",
    "                \"mlp__hidden_layer_sizes\": Integer(10, 1024),\n",
    "                \"mlp__shuffle\": Categorical([True, False]),\n",
    "                \"mlp__alpha\": Real(1.0e-4, 1.0e-1,\n",
    "                                   prior=\"log-uniform\"),\n",
    "                \"mlp__learning_rate_init\": Real(1.0e-3, 1.0e-1,\n",
    "                                                prior=\"log-uniform\"),\n",
    "                \"mlp__batch_size\": Integer(16, 1024)}\n",
    "\n",
    "# Create the pipeline.\n",
    "hpo_pipeline=Pipeline(steps=[('data_scaler', RobustScaler(copy=True)),\n",
    "                             ('mlp', MLPClassifier(solver=\"adam\", max_iter=200,\n",
    "                                                   early_stopping=False))])\n",
    "\n",
    "# Search across different combinations using Bayesian optimization.\n",
    "hpo_search = BayesSearchCV(estimator=hpo_pipeline, search_spaces=search_space, refit=True,\n",
    "                           n_iter=50, cv=5, scoring=\"balanced_accuracy\", verbose=5, n_jobs=5,\n",
    "                           return_train_score=True, random_state=911, error_score=np.nan)\n",
    "# First time.\n",
    "t0 = perf_counter()\n",
    "\n",
    "# Display the model we are optimizing.\n",
    "print(f\"Optimizing ANN model ...\\n\")\n",
    "\n",
    "# Fit the HPO search model.\n",
    "hpo_search.fit(x_train, y_train)\n",
    "\n",
    "# Final time.\n",
    "tf = perf_counter()\n",
    "\n",
    "# Display information.\n",
    "print(f\"HPO finished in {tf-t0:.2f} seconds.\\n\")\n",
    "\n",
    "# Display best parameters.\n",
    "print(f\" Best score {hpo_search.best_score_}, with parameters are: \\n\")\n",
    "for b_param, b_value in hpo_search.best_params_.items():\n",
    "    print(f\" {b_param} --> {b_value}\")\n",
    "# _end_for_\n",
    "\n",
    "# Save the best model.\n",
    "joblib.dump(hpo_search.best_estimator_,\n",
    "            Path(f\"../models/HPO_{train_type}_{test_type}_CV.model\"),\n",
    "            compress=\"zlib\")\n",
    "\n",
    "# Display final message.\n",
    "print(\" Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca07da-0a54-4766-a7a7-5099b3f3c090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " CSD->CSD\n",
    " --------\n",
    " Best score 0.9838429768249363, with parameters are: \n",
    "\n",
    " mlp__activation --> tanh\n",
    " mlp__alpha --> 0.1\n",
    " mlp__batch_size --> 1024\n",
    " mlp__hidden_layer_sizes --> 1024\n",
    " mlp__learning_rate_init --> 0.0013556613475402466\n",
    " mlp__shuffle --> True\n",
    "\n",
    " PDB->PDB\n",
    " --------\n",
    " Best score 0.886341131565478, with parameters are: \n",
    "\n",
    " mlp__activation --> tanh\n",
    " mlp__alpha --> 0.0001\n",
    " mlp__batch_size --> 762\n",
    " mlp__hidden_layer_sizes --> 1024\n",
    " mlp__learning_rate_init --> 0.001\n",
    " mlp__shuffle --> True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc2f02-2acc-419d-a6d4-e628ef5ab318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8cda918-5341-40d6-94af-36ad24fa9d46",
   "metadata": {},
   "source": [
    "### ML model: SciKit ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "567e88c1-d8c6-4ab5-a911-4aaf09b79b0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a model.\n",
    "'''\n",
    "clf_model = Pipeline(steps=[(\"data-scaler\", RobustScaler(copy=True)),\n",
    "                            (\"mlp\", MLPClassifier(hidden_layer_sizes=(1024,),\n",
    "                                                  shuffle=True,\n",
    "                                                  batch_size=1024,\n",
    "                                                  activation=\"relu\",\n",
    "                                                  solver=\"adam\",\n",
    "                                                  alpha=0.1,\n",
    "                                                  learning_rate_init=0.0013556613475402466,\n",
    "                                                  epsilon=1.0e-5,\n",
    "                                                  max_iter=15000,\n",
    "                                                  random_state=0,\n",
    "                                                  verbose=0))])\n",
    "'''\n",
    "# Or, load an HPO model.\n",
    "clf_model = joblib.load(Path(f\"../models/HPO_{train_type}_{test_type}_CV.model\"))\n",
    "# clf_model = joblib.load(Path(f\"../models/CSD_PDB_2022_07_08_10_45_49.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69900a70-c02e-49dd-a3e7-840d43dc06ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f4374-a3c3-4bf4-a2bc-eb5443f1fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeit clf_model.predict(x_test[0].reshape(1, -1))\n",
    "# 253 µs ± 8.69 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d9b0fb-049e-4e5f-8f17-d870a65aa6d3",
   "metadata": {},
   "source": [
    "## Cross-validation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51968039-ff68-404c-9a0f-b976d4b6fb41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First time.\n",
    "t0 = perf_counter()\n",
    "\n",
    "# Run the K-Fold cross-validation.\n",
    "cv_scores = cross_val_score(clf_model, x_train, y_train,\n",
    "                            scoring=\"balanced_accuracy\",\n",
    "                            n_jobs=5, verbose=0, cv=5)\n",
    "# Final time.\n",
    "tf = perf_counter()\n",
    "\n",
    "# Display information.\n",
    "print(f\"Finished K-Fold cross-validation in {tf-t0:.2f} seconds.\\n\")\n",
    "\n",
    "# Print the scores.\n",
    "print(\"Cross-validation Scores: \")\n",
    "for i, sc in enumerate(cv_scores, start=1):\n",
    "    print(f\"Fold: {i}, Score= {sc}\")\n",
    "# _end_for_\n",
    "\n",
    "print(f\"Mean={cv_scores.mean():.4f}, STD= {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7378ebc-e6a8-4aa4-aee8-e011ad1f0c8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "CSD->CSD\n",
    "--------\n",
    "Cross-validation Scores (of the HPO model): \n",
    "Fold: 1, Score= 0.9811558773388793\n",
    "Fold: 2, Score= 0.9835962285170920\n",
    "Fold: 3, Score= 0.9833716168956926\n",
    "Fold: 4, Score= 0.9837192613894198\n",
    "Fold: 5, Score= 0.9823621694776444\n",
    "Mean=0.9828, STD= 0.0010\n",
    "\n",
    "PDB->PDB\n",
    "--------\n",
    "Cross-validation Scores (of the HPO model): \n",
    "Fold: 1, Score= 0.8607687872184141\n",
    "Fold: 2, Score= 0.8998860039715499\n",
    "Fold: 3, Score= 0.8155114226375908\n",
    "Fold: 4, Score= 0.9071965187420980\n",
    "Fold: 5, Score= 0.8880859582182694\n",
    "Mean=0.8743, STD= 0.0334\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e30f1-151b-4b73-84b7-de148e3e496a",
   "metadata": {},
   "source": [
    "## Fit the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9155d5-3257-42ac-b2c6-b9dbb39e8ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display information.\n",
    "print(\"Started fitting ANN model ...\")\n",
    "\n",
    "# First time. \n",
    "t0 = perf_counter()\n",
    "\n",
    "# Fit the model.\n",
    "# clf_model.fit(x_train, y_train)\n",
    "\n",
    "# Second time.\n",
    "t1 = perf_counter()\n",
    "\n",
    "# Display information.\n",
    "print(f\"Finished fitting the training set: {x_train.shape} in {t1-t0:.2f} seconds.\\n\")\n",
    "\n",
    "# Get the predictions on the test set.\n",
    "y_pred = clf_model.predict(x_test)\n",
    "\n",
    "# Third time.\n",
    "t2 = perf_counter()\n",
    "\n",
    "# Display information.\n",
    "print(f\"Finished predicting test set: {x_test.shape}, in {t2-t1:.2f} seconds.\\n\")\n",
    "\n",
    "# Get the final report.\n",
    "clf_rep = classification_report(y_test, y_pred, output_dict=True,\n",
    "                                target_names=CLASS_TARGETS.keys())\n",
    "# Create a large figure.\n",
    "_, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# Create a Confusion matrix.\n",
    "matrix = ConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax, cmap=plt.cm.GnBu,\n",
    "                                                 xticks_rotation=45.0, normalize=\"true\",\n",
    "                                                 display_labels=CLASS_TARGETS.keys(),\n",
    "                                                 values_format=\".2f\", colorbar=False)\n",
    "# Setup the title.\n",
    "ax.set_title(\"Balanced Accuracy = {0:.2f}%, F1-weighted = {1:.2f}%\".format(100.0 * balanced_accuracy_score(y_test, y_pred),\n",
    "                                                                           100.0 * clf_rep[\"weighted avg\"][\"f1-score\"]), fontsize=18)\n",
    "ax.set_xlabel(\"Predicted label\", fontsize=16)\n",
    "ax.set_ylabel(\"True label\", fontsize=16)\n",
    "\n",
    "# Set the custom colorbar.\n",
    "matrix.im_.autoscale()\n",
    "plt.colorbar(matrix.im_, fraction=0.046, pad=0.04)\n",
    "\n",
    "# Save the figure.\n",
    "plt.savefig(Path(output_path/\"plots\"/f\"{train_type}_{test_type}_confusion_matrix.png\"),\n",
    "            bbox_inches=\"tight\", dpi=300, facecolor='w', edgecolor='w',\n",
    "            orientation='landscape')\n",
    "\n",
    "# Save the classification to a json file.\n",
    "with open(Path(output_path/\"reports\"/f\"{train_type}_{test_type}_classification_report.json\"),\n",
    "          mode='w') as json_file:\n",
    "    json.dump(clf_rep, json_file, indent=4)\n",
    "# _end_with_\n",
    "\n",
    "# Get the timestamp.\n",
    "date_now = datetime.now().strftime(\"%Y_%m_%d_%I_%M_%S\")\n",
    "\n",
    "# Save the trained model.\n",
    "joblib.dump(clf_model,\n",
    "            Path(f\"../models/{train_type}_{test_type}_{date_now}.joblib\"),\n",
    "            compress=4)\n",
    "\n",
    "# Get the loss-curve.\n",
    "loss_curve = clf_model[\"mlp\"].loss_curve_\n",
    "\n",
    "# Plot the figure.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(loss_curve)\n",
    "ax.set_title(f\"Multi Layer Perceptron Classifier\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the figure.\n",
    "plt.savefig(Path(output_path/\"plots\"/f\"{train_type}_{test_type}_loss_curve.png\"),\n",
    "            bbox_inches=\"tight\", facecolor='w', edgecolor='w',\n",
    "            dpi=300, orientation=\"landscape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded86ecb-7ec9-4cba-a718-5f02330007df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac122a8-800c-4955-a450-b68b0e21cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_= pd.DataFrame(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "FP = cm_.sum(axis=0) - np.diag(cm_)  \n",
    "FN = cm_.sum(axis=1) - np.diag(cm_)\n",
    "TP = np.diag(cm_)\n",
    "TN = cm_.values.sum() - (FP + FN + TP)\n",
    "\n",
    "# Support (number of instances of each class)\n",
    "SUP = cm_.sum(axis=1)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "\n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "\n",
    "# F1-Score\n",
    "F1S = 2.0 * (PPV * TPR) / (PPV + TPR)\n",
    "\n",
    "df = pd.DataFrame({\"F1S\": F1S, \"TPR\": TPR, \"TNR\": TNR, \"PPV\": PPV,\n",
    "                   \"NPV\": NPV, \"FPR\": FPR, \"FNR\": FNR, \"FDR\": FDR})\n",
    "\n",
    "df.index = list(CLASS_TARGETS.keys())\n",
    "\n",
    "df.to_csv(Path(f\"../results/reports/{train_type}_{test_type}_per_class.csv\"))\n",
    "\n",
    "print(df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7927a300-6976-4daa-88ee-23ed1ad598b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f305cb4-7313-48a2-a5e0-e9609777e8c0",
   "metadata": {},
   "source": [
    "## Probability bar-plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be01b69c-0f60-45de-b9ae-88d8475efcd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the predicted probabilities on the test set.\n",
    "y_probs = clf_model.predict_proba(x_test)\n",
    "\n",
    "# Store the probabilities per class.\n",
    "probs_per_class = defaultdict(list)\n",
    "\n",
    "# Group result by true class.\n",
    "for v, u in zip(y_test, y_probs):\n",
    "    probs_per_class[v].append(u)\n",
    "# _end_for_\n",
    "\n",
    "# Get the class labels.\n",
    "class_label = list(CLASS_TARGETS.keys())\n",
    "\n",
    "# Create a new figure.\n",
    "fig, ax = plt.subplots(nrows=2, ncols=4,\n",
    "                       figsize=(18, 12))\n",
    "\n",
    "# Delete the last plot.\n",
    "fig.delaxes(ax[1][3])\n",
    "\n",
    "# Put all the bar-plots together.\n",
    "for c_num, c_label in enumerate(probs_per_class, start=0):\n",
    "        \n",
    "    # Enable the subplot.\n",
    "    plt.subplot(2, 4, c_num+1)\n",
    "    \n",
    "    # Get the proabilities for each predicted class.\n",
    "    prob_c = np.sum(probs_per_class[c_num], axis=0)/len(probs_per_class[c_num])\n",
    "    \n",
    "    # Get the indexes of the sorted vector (descending order).\n",
    "    descending_order = np.argsort(prob_c)[::-1]\n",
    "    \n",
    "    # Initial assignments of bar entries.\n",
    "    heights = prob_c[descending_order]\n",
    "    \n",
    "    # Exclude the zero entries.\n",
    "    not_equal_zero = heights!=0.0\n",
    "    \n",
    "    # Compute the entropy:\n",
    "    En = -np.sum(heights[not_equal_zero]*np.log(heights[not_equal_zero]))\n",
    "    \n",
    "    # Get the relevant colours.\n",
    "    colors_, labels_ = [], []\n",
    "    for k in descending_order:\n",
    "        colors_.append(plt.cm.tab20.colors[k])\n",
    "        labels_.append(class_label[k])\n",
    "    # _end_for_\n",
    "        \n",
    "    # Plot the normalized probabilites.\n",
    "    bars = plt.bar(x=labels_, height=heights, color=colors_)\n",
    "    \n",
    "    # Access the bar attributes to place\n",
    "    # the text in the appropriate location.\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x(), yval + .002,\n",
    "                 f\"{np.round(yval, 3):.3f}\",\n",
    "                 fontsize='x-small')\n",
    "    # _end_for_\n",
    "    \n",
    "    # Setup grid and more.\n",
    "    plt.grid()\n",
    "    plt.ylim([0.0, 1.0])\n",
    "\n",
    "    plt.ylabel(\"Probability\", fontsize=12)\n",
    "    plt.xlabel(\"Predicted class\", fontsize=12)\n",
    "    \n",
    "    plt.title(f\"True class: {class_label[c_num]}, Entropy {En:.4f}\", fontsize=12)\n",
    "# _end_for_\n",
    "\n",
    "# Set the spacing between subplots.\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "# Save the figure.\n",
    "plt.savefig(Path(output_path/\"plots\"/f\"{train_type}_{test_type}_probablities_entropy.png\"),\n",
    "            bbox_inches=\"tight\", dpi=300, facecolor='w', edgecolor='w',\n",
    "            orientation='landscape');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75b835",
   "metadata": {},
   "source": [
    "### End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e1398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_25",
   "language": "python",
   "name": "tensorflow_25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
